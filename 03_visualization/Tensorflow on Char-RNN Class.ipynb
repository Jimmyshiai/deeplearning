{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Tensorflow on Char-RNN\n",
    "#### This program was created based on the following links: \n",
    "https://github.com/patrickcnkm/tensorflow-char-rnn\n",
    "\n",
    "\n",
    "####  The target of this coding is:\n",
    "- Understanding how the weights and biases are being produced and changed in each layer, then get more sense how the RNN is working, and consequently get further understanding on GRU and LSTM.\n",
    "\n",
    "Since this code will be visualized by tensorboard, it is proposed to start tensorboard in the folder where this code is located in the command windows by the following command:\n",
    "> tensorboard --logdir=logs/\n",
    "\n",
    "The usage of tensorboard can be found in the following link:\n",
    "https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have TensorFlow version 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import logging\n",
    "# Disable Tensorflow logging messages.\n",
    "#logging.getLogger('tensorflow').setLevel(logging.WARNING)\n",
    "\n",
    "# This code was tested with TensorFlow v1.4\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the class of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-78-aba96088d5c5>, line 251)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-78-aba96088d5c5>\"\u001b[0;36m, line \u001b[0;32m251\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Disable Tensorflow logging messages.\n",
    "#logging.getLogger('tensorflow').setLevel(logging.WARNING)\n",
    "\n",
    "class CharRNN(object):\n",
    "  \"\"\"Character RNN model.\"\"\"\n",
    "  \n",
    "  def __init__(self, is_training, batch_size, num_unrollings, vocab_size, \n",
    "               hidden_size, max_grad_norm, embedding_size, num_layers,\n",
    "               learning_rate, model, dropout=0.0, input_dropout=0.0, use_batch=True):\n",
    "    self.batch_size = batch_size\n",
    "    self.num_unrollings = num_unrollings\n",
    "    if not use_batch:\n",
    "      self.batch_size = 1\n",
    "      self.num_unrollings = 1\n",
    "    self.hidden_size = hidden_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.max_grad_norm = max_grad_norm\n",
    "    self.num_layers = num_layers\n",
    "    self.embedding_size = embedding_size\n",
    "    self.model = model\n",
    "    self.dropout = dropout\n",
    "    self.input_dropout = input_dropout\n",
    "    if embedding_size <= 0:\n",
    "      self.input_size = vocab_size\n",
    "      # Don't do dropout on one hot representation.\n",
    "      self.input_dropout = 0.0\n",
    "    else:\n",
    "      self.input_size = embedding_size\n",
    "    self.model_size = (embedding_size * vocab_size + # embedding parameters\n",
    "                       # lstm parameters\n",
    "                       4 * hidden_size * (hidden_size + self.input_size + 1) +\n",
    "                       # softmax parameters\n",
    "                       vocab_size * (hidden_size + 1) +\n",
    "                       # multilayer lstm parameters for extra layers.\n",
    "                       (num_layers - 1) * 4 * hidden_size *\n",
    "                       (hidden_size + hidden_size + 1))\n",
    "    # self.decay_rate = decay_rate\n",
    "\n",
    "    # Placeholder to feed in input and targets/labels data.\n",
    "    self.input_data = tf.placeholder(tf.int64,\n",
    "                                     [self.batch_size, self.num_unrollings],\n",
    "                                     name='inputs')\n",
    "    self.targets = tf.placeholder(tf.int64,\n",
    "                                  [self.batch_size, self.num_unrollings],\n",
    "                                  name='targets')\n",
    "\n",
    "    if self.model == 'rnn':\n",
    "      cell_fn = tf.contrib.rnn.BasicRNNCell\n",
    "    elif self.model == 'lstm':\n",
    "      cell_fn = tf.contrib.rnn.BasicLSTMCell\n",
    "    elif self.model == 'gru':\n",
    "      cell_fn = tf.contrib.rnn.GRUCell\n",
    "\n",
    "    # params = {'input_size': self.input_size}\n",
    "    params = {}\n",
    "    if self.model == 'lstm':\n",
    "      # add bias to forget gate in lstm.\n",
    "      params['forget_bias'] = 0.0\n",
    "      params['state_is_tuple'] = True\n",
    "    # Create multilayer cell.\n",
    "    cell = cell_fn(\n",
    "        self.hidden_size, reuse=tf.get_variable_scope().reuse,\n",
    "        **params)\n",
    "\n",
    "    cells = [cell]\n",
    "    # params['input_size'] = self.hidden_size\n",
    "    # more explicit way to create cells for MultiRNNCell than\n",
    "    # [higher_layer_cell] * (self.num_layers - 1)\n",
    "    for i in range(self.num_layers-1):\n",
    "      higher_layer_cell = cell_fn(\n",
    "          self.hidden_size, reuse=tf.get_variable_scope().reuse,\n",
    "          **params)\n",
    "      cells.append(higher_layer_cell)\n",
    "\n",
    "    if is_training and self.dropout > 0:\n",
    "      cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "        cell,\n",
    "        output_keep_prob=1.0-self.dropout)\n",
    "               for cell in cells]\n",
    "\n",
    "    multi_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    with tf.name_scope('initial_state'):\n",
    "\n",
    "      # zero_state is used to compute the intial state for cell.\n",
    "      self.zero_state = multi_cell.zero_state(self.batch_size, tf.float32)\n",
    "      # Placeholder to feed in initial state.\n",
    "      # self.initial_state = tf.placeholder(\n",
    "      #   tf.float32,\n",
    "      #   [self.batch_size, multi_cell.state_size],\n",
    "      #   'initial_state')\n",
    "\n",
    "      self.initial_state = create_tuple_placeholders_with_default(\n",
    "          multi_cell.zero_state(batch_size, tf.float32),\n",
    "          extra_dims=(None,),\n",
    "          shape=multi_cell.state_size)\n",
    "      \n",
    "\n",
    "    # Embeddings layers.\n",
    "    with tf.name_scope('embedding_layer'):\n",
    "      if embedding_size > 0:\n",
    "        self.embedding = tf.get_variable(\n",
    "          'embedding', [self.vocab_size, self.embedding_size])\n",
    "      else:\n",
    "        self.embedding = tf.constant(np.eye(self.vocab_size), dtype=tf.float32)\n",
    "\n",
    "      inputs = tf.nn.embedding_lookup(self.embedding, self.input_data)\n",
    "      if is_training and self.input_dropout > 0:\n",
    "        inputs = tf.nn.dropout(inputs, 1 - self.input_dropout)\n",
    "\n",
    "    with tf.name_scope('slice_inputs'):\n",
    "      # Slice inputs into a list of shape [batch_size, 1] data colums.\n",
    "      sliced_inputs = [tf.squeeze(input_, [1])\n",
    "                       for input_ in tf.split(axis=1, num_or_size_splits=self.num_unrollings, value=inputs)]\n",
    "      \n",
    "    # Copy cell to do unrolling and collect outputs.\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(\n",
    "      multi_cell, sliced_inputs,\n",
    "      initial_state=self.initial_state)\n",
    "\n",
    "    self.final_state = final_state\n",
    "\n",
    "    with tf.name_scope('flatten_ouputs'):\n",
    "      # Flatten the outputs into one dimension.\n",
    "      flat_outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, hidden_size])\n",
    "\n",
    "    with tf.name_scope('flatten_targets'):\n",
    "      # Flatten the targets too.\n",
    "      flat_targets = tf.reshape(tf.concat(axis=1, values=self.targets), [-1])\n",
    "    \n",
    "    # Create softmax parameters, weights and bias.\n",
    "    with tf.variable_scope('softmax') as sm_vs:\n",
    "      softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, vocab_size])\n",
    "      softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "      self.logits = tf.matmul(flat_outputs, softmax_w) + softmax_b\n",
    "      self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "      # Compute mean cross entropy loss for each output.\n",
    "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=self.logits, labels=flat_targets)\n",
    "      self.mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    with tf.name_scope('loss_monitor'):\n",
    "      # Count the number of elements and the sum of mean_loss\n",
    "      # from each batch to compute the average loss.\n",
    "      count = tf.Variable(1.0, name='count')\n",
    "      sum_mean_loss = tf.Variable(1.0, name='sum_mean_loss')\n",
    "      \n",
    "      self.reset_loss_monitor = tf.group(sum_mean_loss.assign(0.0),\n",
    "                                         count.assign(0.0),\n",
    "                                         name='reset_loss_monitor')\n",
    "      self.update_loss_monitor = tf.group(sum_mean_loss.assign(sum_mean_loss +\n",
    "                                                               self.mean_loss),\n",
    "                                          count.assign(count + 1),\n",
    "                                          name='update_loss_monitor')\n",
    "      with tf.control_dependencies([self.update_loss_monitor]):\n",
    "        self.average_loss = sum_mean_loss / count\n",
    "        self.ppl = tf.exp(self.average_loss)\n",
    "\n",
    "      # Monitor the loss.\n",
    "      loss_summary_name = \"average loss\"\n",
    "      ppl_summary_name = \"perplexity\"\n",
    "  \n",
    "      average_loss_summary = tf.summary.scalar(loss_summary_name, self.average_loss)\n",
    "      ppl_summary = tf.summary.scalar(ppl_summary_name, self.ppl)\n",
    "\n",
    "    # Monitor the loss.\n",
    "    self.summaries = tf.summary.merge([average_loss_summary, ppl_summary],\n",
    "                                      name='loss_monitor')\n",
    "    \n",
    "    self.global_step = tf.get_variable('global_step', [],\n",
    "                                       initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    self.learning_rate = tf.constant(learning_rate)\n",
    "    if is_training:\n",
    "      # learning_rate = tf.train.exponential_decay(1.0, self.global_step,\n",
    "      #                                            5000, 0.1, staircase=True)\n",
    "      tvars = tf.trainable_variables()\n",
    "      grads, _ = tf.clip_by_global_norm(tf.gradients(self.mean_loss, tvars),\n",
    "                                        self.max_grad_norm)\n",
    "      # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      # optimizer = tf.train.RMSPropOptimizer(learning_rate, decay_rate)\n",
    "      optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "      self.train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                                global_step=self.global_step)\n",
    "      \n",
    "  def run_epoch(self, session, data_size, batch_generator, is_training,\n",
    "                verbose=0, freq=10, summary_writer=None, debug=False, divide_by_n=1):\n",
    "    \"\"\"Runs the model on the given data for one full pass.\"\"\"\n",
    "    # epoch_size = ((data_size // self.batch_size) - 1) // self.num_unrollings\n",
    "    epoch_size = data_size // (self.batch_size * self.num_unrollings)\n",
    "    if data_size % (self.batch_size * self.num_unrollings) != 0:\n",
    "        epoch_size += 1\n",
    "\n",
    "    if verbose > 0:\n",
    "        logging.info('epoch_size: %d', epoch_size)\n",
    "        logging.info('data_size: %d', data_size)\n",
    "        logging.info('num_unrollings: %d', self.num_unrollings)\n",
    "        logging.info('batch_size: %d', self.batch_size)\n",
    "\n",
    "    if is_training:\n",
    "        extra_op = self.train_op\n",
    "    else:\n",
    "        extra_op = tf.no_op()\n",
    "\n",
    "    # Prepare initial state and reset the average loss\n",
    "    # computation.\n",
    "    state = session.run(self.zero_state)\n",
    "    self.reset_loss_monitor.run()\n",
    "    start_time = time.time()\n",
    "    for step in range(epoch_size // divide_by_n):\n",
    "        # Generate the batch and use [:-1] as inputs and [1:] as targets.\n",
    "        data = batch_generator.next()\n",
    "        inputs = np.array(data[:-1]).transpose()\n",
    "        targets = np.array(data[1:]).transpose()\n",
    "        \n",
    "        ops = [self.average_loss, self.final_state, extra_op,\n",
    "               self.summaries, self.global_step, self.learning_rate]\n",
    "        feed_dict = {self.input_data: inputs, self.targets: targets,\n",
    "                     self.initial_state: state}\n",
    "        results = session.run(ops, feed_dict)\n",
    "        average_loss, state, _, summary_str, global_step, lr = results\n",
    "        ppl = np.exp(average_loss)\n",
    "        if (verbose > 0) and ((step+1) % freq == 0):\n",
    "            logging.info(\"%.1f%%, step:%d, perplexity: %.3f, speed: %.0f words\",\n",
    "                         (step + 1) * 1.0 / epoch_size * 100, step, ppl,\n",
    "                         (step + 1) * self.batch_size * self.num_unrollings /\n",
    "                         (time.time() - start_time))\n",
    "        logging.info(\"Perplexity: %.3f, speed: %.0f words per sec\",\n",
    "                     ppl, (step + 1) * self.batch_size * self.num_unrollings /\n",
    "                     (time.time() - start_time))\n",
    "        return ppl, summary_str, global_step\n",
    "\n",
    "  def sample_seq(self, session, length, start_text, vocab_index_dict,\n",
    "                 index_vocab_dict, temperature=1.0, max_prob=True):\n",
    "\n",
    "    state = session.run(self.zero_state)\n",
    "\n",
    "    # use start_text to warm up the RNN.\n",
    "    if start_text is not None and len(start_text) > 0:\n",
    "        seq = list(start_text)\n",
    "        for char in start_text[:-1]:\n",
    "            x = np.array([[char2id(char, vocab_index_dict)]])\n",
    "            print(x)\n",
    "            state = session.run(self.final_state,\n",
    "                                {self.input_data: x,\n",
    "                                 self.initial_state: state})\n",
    "    x = np.array([[char2id(start_text[-1], vocab_index_dict)]])\n",
    "    else:\n",
    "        vocab_size = len(vocab_index_dict.keys())\n",
    "        x = np.array([[np.random.randint(0, vocab_size)]])\n",
    "        seq = []\n",
    "\n",
    "    for i in range(length):\n",
    "        state, logits = session.run([self.final_state,\n",
    "                                     self.logits],\n",
    "                                    {self.input_data: x,\n",
    "                                     self.initial_state: state})\n",
    "        unnormalized_probs = np.exp((logits - np.max(logits)) / temperature)\n",
    "        probs = unnormalized_probs / np.sum(unnormalized_probs)\n",
    "\n",
    "        if max_prob:\n",
    "            sample = np.argmax(probs[0])\n",
    "        else:\n",
    "            sample = np.random.choice(self.vocab_size, 1, p=probs[0])[0]\n",
    "\n",
    "        seq.append(id2char(sample, index_vocab_dict))\n",
    "        x = np.array([[sample]])\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id('l', vocab_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the class of batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \"\"\"Generate and hold batches.\"\"\"\n",
    "\n",
    "    def __init__(self, text, batch_size, n_unrollings, vocab_size,\n",
    "                 vocab_index_dict, index_vocab_dict):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self._n_unrollings = n_unrollings\n",
    "        self.vocab_index_dict = vocab_index_dict\n",
    "        self.index_vocab_dict = index_vocab_dict\n",
    "\n",
    "        segment = self._text_size // batch_size\n",
    "\n",
    "        # number of elements in cursor list is the same as\n",
    "        # batch_size.  each batch is just the collection of\n",
    "        # elements in where the cursors are pointing to.\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = char2id(self._text[self._cursor[b]], self.vocab_index_dict)\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._n_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batches2string(batches, index_vocab_dict):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, id2char_list(b, index_vocab_dict))]\n",
    "  return s\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def char2id(char, vocab_index_dict):\n",
    "  try:\n",
    "    return vocab_index_dict[char]\n",
    "  except KeyError:\n",
    "    logging.info('Unexpected char %s', char)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def id2char(index, index_vocab_dict):\n",
    "  return index_vocab_dict[index]\n",
    "\n",
    "    \n",
    "def id2char_list(lst, index_vocab_dict):\n",
    "  return [id2char(i, index_vocab_dict) for i in lst]\n",
    "\n",
    "\n",
    "def create_tuple_placeholders_with_default(inputs, extra_dims, shape):\n",
    "\n",
    "  if isinstance(shape, int):\n",
    "    result = tf.placeholder_with_default(\n",
    "      inputs, list(extra_dims) + [shape])\n",
    "  else:\n",
    "    subplaceholders = [create_tuple_placeholders_with_default(\n",
    "      subinputs, extra_dims, subshape)\n",
    "                       for subinputs, subshape in zip(inputs, shape)]\n",
    "    t = type(shape)\n",
    "    if t == tuple:\n",
    "      result = t(subplaceholders)\n",
    "    else:\n",
    "      result = t(*subplaceholders)    \n",
    "  return result\n",
    "\n",
    "        \n",
    "def create_tuple_placeholders(dtype, extra_dims, shape):\n",
    "  if isinstance(shape, int):\n",
    "    result = tf.placeholder(dtype, list(extra_dims) + [shape])\n",
    "  else:\n",
    "    subplaceholders = [create_tuple_placeholders(dtype, extra_dims, subshape)\n",
    "                       for subshape in shape]\n",
    "    t = type(shape)\n",
    "\n",
    "    # Handles both tuple and LSTMStateTuple.\n",
    "    if t == tuple:\n",
    "      result = t(subplaceholders)\n",
    "    else:\n",
    "      result = t(*subplaceholders)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_batch=True\n",
    "batch_size = 20\n",
    "model='rnn'\n",
    "num_unrollings = 10\n",
    "is_training=True\n",
    "if not use_batch:\n",
    "    batch_size = 1\n",
    "    num_unrollings = 1\n",
    "hidden_size = 128\n",
    "#vocab_size = 1000\n",
    "max_grad_norm = 5\n",
    "num_layers = 2\n",
    "embedding_size = 0\n",
    "dropout = 0.0\n",
    "input_dropout = 0.0\n",
    "num_epochs = 50\n",
    "valid_frac = 0.05\n",
    "train_frac = 0.05\n",
    "learning_rate = 2e-3\n",
    "decay_rate = 0.95\n",
    "verbose = 0\n",
    "best_valid_ppl = np.inf\n",
    "encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab(text):\n",
    "    unique_chars = list(set(text))\n",
    "    vocab_size = len(unique_chars)\n",
    "    vocab_index_dict = {}\n",
    "    index_vocab_dict = {}\n",
    "    for i, char in enumerate(unique_chars):\n",
    "        vocab_index_dict[char] = i\n",
    "        index_vocab_dict[i] = char\n",
    "    return vocab_index_dict, index_vocab_dict, vocab_size\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file, encoding):\n",
    "    with codecs.open(vocab_file, 'r', encoding=encoding) as f:\n",
    "        vocab_index_dict = json.load(f)\n",
    "    index_vocab_dict = {}\n",
    "    vocab_size = 0\n",
    "    for char, index in iteritems(vocab_index_dict):\n",
    "        index_vocab_dict[index] = char\n",
    "        vocab_size += 1\n",
    "    return vocab_index_dict, index_vocab_dict, vocab_size\n",
    "\n",
    "\n",
    "def save_vocab(vocab_index_dict, vocab_file, encoding):\n",
    "    with codecs.open(vocab_file, 'w', encoding=encoding) as f:\n",
    "        json.dump(vocab_index_dict, f, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary based on given text\n",
    "text='I like apple'\n",
    "logging.info('Creating vocabulary')\n",
    "vocab_index_dict, index_vocab_dict, vocab_size = create_vocab(text)\n",
    "logging.info('Vocab size: %d', vocab_size)\n",
    "train_size = int(train_frac * len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare for the parameters to create model\n",
    "params = {'batch_size': batch_size,\n",
    "          'num_unrollings': num_unrollings,\n",
    "          'hidden_size': hidden_size,\n",
    "          'max_grad_norm': max_grad_norm,\n",
    "          'embedding_size': embedding_size,\n",
    "          'num_layers': num_layers,\n",
    "          'learning_rate': learning_rate,\n",
    "          'model': model,\n",
    "          'dropout': dropout,\n",
    "          'input_dropout': input_dropout,\n",
    "          'vocab_size':vocab_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name average loss is illegal; using average_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name average loss is illegal; using average_loss instead.\n"
     ]
    }
   ],
   "source": [
    "# Create graphs\n",
    "logging.info('Creating graph')\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('training'):\n",
    "        train_model = CharRNN(is_training=True, use_batch=True, **params)\n",
    "        tf.get_variable_scope().reuse_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info('Model size (number of parameters): %s\\n', train_model.model_size)\n",
    "logging.info('Start training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Epoch %d===================\n",
      " 1\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 2\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 3\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 4\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 5\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 6\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 7\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 8\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 9\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 10\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 11\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 12\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 13\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 14\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 15\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 16\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 17\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 18\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 19\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 20\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 21\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 22\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 23\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 24\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 25\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 26\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 27\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 28\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 29\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 30\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 31\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 32\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 33\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 34\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 35\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 36\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 37\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 38\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 39\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 40\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 41\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 42\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 43\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 44\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 45\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 46\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 47\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 48\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 49\n",
      "Training on training set\n",
      "=================== Epoch %d===================\n",
      " 50\n",
      "Training on training set\n"
     ]
    }
   ],
   "source": [
    "# Create batch generators.\n",
    "batch_size = params['batch_size']\n",
    "num_unrollings = params['num_unrollings']\n",
    "train_batches = BatchGenerator(text, batch_size, num_unrollings, vocab_size, \n",
    "                               vocab_index_dict, index_vocab_dict)\n",
    "progress_freq=100\n",
    "num_epochs=50\n",
    "verbose=0\n",
    "n_save=1\n",
    "with tf.Session(graph=graph) as session:\n",
    "    graph_info = session.graph\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(num_epochs):\n",
    "        print('=' * 19 + ' Epoch %d' + '=' * 19 + '\\n', i+1)\n",
    "        print('Training on training set')\n",
    "        # training step\n",
    "        #ppl, train_summary_str, global_step = train_model.run_epoch(\n",
    "        train_model.run_epoch(\n",
    "                        session,\n",
    "                        train_size,\n",
    "                        train_batches,\n",
    "                        is_training=True,\n",
    "                        verbose=verbose,\n",
    "                        freq=progress_freq,\n",
    "                        divide_by_n=n_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 1) for Tensor 'training/inputs:0', which has shape '(20, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-35b1b6541cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mvocab_index_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_vocab_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                     max_prob=False)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sampled text is:\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-f2bf9d56bab9>\u001b[0m in \u001b[0;36msample_seq\u001b[0;34m(self, session, length, start_text, vocab_index_dict, index_vocab_dict, temperature, max_prob)\u001b[0m\n\u001b[1;32m    247\u001b[0m             state = session.run(self.final_state,\n\u001b[1;32m    248\u001b[0m                                 {self.input_data: x,\n\u001b[0;32m--> 249\u001b[0;31m                                  self.initial_state: state})\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 1) for Tensor 'training/inputs:0', which has shape '(20, 10)'"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "# Sampling a sequence \n",
    "with tf.Session(graph=graph) as session:\n",
    "    sample = train_model.sample_seq(session, 4, \"hi\",\n",
    "                                    vocab_index_dict, index_vocab_dict,\n",
    "                                    temperature=1,\n",
    "                                    max_prob=False)\n",
    "    print('Sampled text is:\\n%s' % sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
