{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Tensorflow on Char-RNN\n",
    "#### This program was created based on the following links: \n",
    "https://github.com/patrickcnkm/tensorflow-char-rnn\n",
    "\n",
    "\n",
    "####  The target of this coding is:\n",
    "- Understanding how the weights and biases are being produced and changed in each layer, then get more sense how the RNN is working, and consequently get further understanding on GRU and LSTM.\n",
    "\n",
    "Since this code will be visualized by tensorboard, it is proposed to start tensorboard in the folder where this code is located in the command windows by the following command:\n",
    "> tensorboard --logdir=logs/\n",
    "\n",
    "The usage of tensorboard can be found in the following link:\n",
    "https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have TensorFlow version 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import logging\n",
    "# Disable Tensorflow logging messages.\n",
    "#logging.getLogger('tensorflow').setLevel(logging.WARNING)\n",
    "\n",
    "# This code was tested with TensorFlow v1.4\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Initialize the parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 9,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "use_batch=True\n",
    "batch_size = 20\n",
    "model='rnn'\n",
    "num_unrollings = 10\n",
    "is_training=True\n",
    "if not use_batch:\n",
    "    batch_size = 1\n",
    "    num_unrollings = 1\n",
    "hidden_size = 128\n",
    "#vocab_size = 1000\n",
    "max_grad_norm = 5\n",
    "num_layers = 2\n",
    "embedding_size = 0\n",
    "dropout = 0.0\n",
    "input_dropout = 0.0\n",
    "num_epochs = 50\n",
    "valid_frac = 0.05\n",
    "train_frac = 0.05\n",
    "learning_rate = 2e-3\n",
    "decay_rate = 0.95\n",
    "verbose = 0\n",
    "best_valid_ppl = np.inf\n",
    "encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab(text):\n",
    "    unique_chars = list(set(text))\n",
    "    vocab_size = len(unique_chars)\n",
    "    vocab_index_dict = {}\n",
    "    index_vocab_dict = {}\n",
    "    for i, char in enumerate(unique_chars):\n",
    "        vocab_index_dict[char] = i\n",
    "        index_vocab_dict[i] = char\n",
    "    return vocab_index_dict, index_vocab_dict, vocab_size\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file, encoding):\n",
    "    with codecs.open(vocab_file, 'r', encoding=encoding) as f:\n",
    "        vocab_index_dict = json.load(f)\n",
    "    index_vocab_dict = {}\n",
    "    vocab_size = 0\n",
    "    for char, index in iteritems(vocab_index_dict):\n",
    "        index_vocab_dict[index] = char\n",
    "        vocab_size += 1\n",
    "    return vocab_index_dict, index_vocab_dict, vocab_size\n",
    "\n",
    "\n",
    "def save_vocab(vocab_index_dict, vocab_file, encoding):\n",
    "    with codecs.open(vocab_file, 'w', encoding=encoding) as f:\n",
    "        json.dump(vocab_index_dict, f, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vocabulary based on given text\n",
    "text='I like apple'\n",
    "logging.info('Creating vocabulary')\n",
    "vocab_index_dict, index_vocab_dict, vocab_size = create_vocab(text)\n",
    "#params['vocab_size'] = vocab_size\n",
    "logging.info('Vocab size: %d', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if embedding_size <= 0:\n",
    "    input_size = vocab_size\n",
    "    # Don't do dropout on one hot representation\n",
    "    input_dropout = 0.0\n",
    "else:\n",
    "    input_size = embedding_size\n",
    "    model_size = (embedding_size * vocab_size + # embedding parameters\n",
    "                       # lstm parameters\n",
    "                       4 * hidden_size * (hidden_size + input_size + 1) +\n",
    "                       # softmax parameters\n",
    "                       vocab_size * (hidden_size + 1) +\n",
    "                       # multilayer lstm parameters for extra layers.\n",
    "                       (num_layers - 1) * 4 * hidden_size *\n",
    "                       (hidden_size + hidden_size + 1))\n",
    "    # self.decay_rate = decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': batch_size,\n",
    "          'num_unrollings': num_unrollings,\n",
    "          'hidden_size': hidden_size,\n",
    "          'max_grad_norm': max_grad_norm,\n",
    "          'embedding_size': embedding_size,\n",
    "          'num_layers': num_layers,\n",
    "          'learning_rate': learning_rate,\n",
    "          'model': model,\n",
    "          'dropout': dropout,\n",
    "          'input_dropout': input_dropout,\n",
    "          'vocab_size':vocab_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batches2string(batches, index_vocab_dict):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, id2char_list(b, index_vocab_dict))]\n",
    "  return s\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def char2id(char, vocab_index_dict):\n",
    "  try:\n",
    "    return vocab_index_dict[char]\n",
    "  except KeyError:\n",
    "    logging.info('Unexpected char %s', char)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def id2char(index, index_vocab_dict):\n",
    "  return index_vocab_dict[index]\n",
    "\n",
    "    \n",
    "def id2char_list(lst, index_vocab_dict):\n",
    "  return [id2char(i, index_vocab_dict) for i in lst]\n",
    "\n",
    "\n",
    "def create_tuple_placeholders_with_default(inputs, extra_dims, shape):\n",
    "\n",
    "  if isinstance(shape, int):\n",
    "    result = tf.placeholder_with_default(\n",
    "      inputs, list(extra_dims) + [shape])\n",
    "  else:\n",
    "    subplaceholders = [create_tuple_placeholders_with_default(\n",
    "      subinputs, extra_dims, subshape)\n",
    "                       for subinputs, subshape in zip(inputs, shape)]\n",
    "    t = type(shape)\n",
    "    if t == tuple:\n",
    "      result = t(subplaceholders)\n",
    "    else:\n",
    "      result = t(*subplaceholders)    \n",
    "  return result\n",
    "\n",
    "        \n",
    "def create_tuple_placeholders(dtype, extra_dims, shape):\n",
    "  if isinstance(shape, int):\n",
    "    result = tf.placeholder(dtype, list(extra_dims) + [shape])\n",
    "  else:\n",
    "    subplaceholders = [create_tuple_placeholders(dtype, extra_dims, subshape)\n",
    "                       for subshape in shape]\n",
    "    t = type(shape)\n",
    "\n",
    "    # Handles both tuple and LSTMStateTuple.\n",
    "    if t == tuple:\n",
    "      result = t(subplaceholders)\n",
    "    else:\n",
    "      result = t(*subplaceholders)\n",
    "  return result\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-268a72435af5>:24: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-268a72435af5>:24: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "# Placeholder to feed in input and targets/labels data.\n",
    "input_data = tf.placeholder(tf.int64,\n",
    "                            [batch_size, num_unrollings],\n",
    "                            name='inputs')\n",
    "targets = tf.placeholder(tf.int64,\n",
    "                         [batch_size, num_unrollings],\n",
    "                         name='targets')\n",
    "if model == 'rnn':\n",
    "    cell_fn = tf.contrib.rnn.BasicRNNCell\n",
    "elif model == 'lstm':\n",
    "    cell_fn = tf.contrib.rnn.BasicLSTMCell\n",
    "elif model == 'gru':\n",
    "    cell_fn = tf.contrib.rnn.GRUCell\n",
    "\n",
    "# params = {'input_size': input_size}\n",
    "params_1 = {}\n",
    "if model == 'lstm':\n",
    "    # add bias to forget gate in lstm.\n",
    "    params_1['forget_bias'] = 0.0\n",
    "    params_1['state_is_tuple'] = True\n",
    "    # Create multilayer cell.\n",
    "cell = cell_fn(\n",
    "    hidden_size, reuse=tf.get_variable_scope().reuse,\n",
    "    **params_1)\n",
    "\n",
    "cells = [cell]\n",
    "# params['input_size'] = self.hidden_size\n",
    "# more explicit way to create cells for MultiRNNCell than\n",
    "# [higher_layer_cell] * (num_layers - 1)\n",
    "for i in range(num_layers-1):\n",
    "    higher_layer_cell = cell_fn(\n",
    "        hidden_size, reuse=tf.get_variable_scope().reuse,\n",
    "        **params_1)\n",
    "    cells.append(higher_layer_cell)\n",
    "\n",
    "if is_training and dropout > 0:\n",
    "    cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "        cell,\n",
    "        output_keep_prob=1.0-dropout)\n",
    "             for cell in cells]\n",
    "\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 15,
        "hidden": false,
        "row": 8,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name average loss is illegal; using average_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name average loss is illegal; using average_loss instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.name_scope('initial_state'):\n",
    "    zero_state = multi_cell.zero_state(batch_size, tf.float32)\n",
    "    #subplaceholders=[]\n",
    "        # zero_state is used to compute the intial state for cell.\n",
    "    \n",
    "  # Placeholder to feed in initial state.\n",
    "  #initial_state = tf.placeholder(\n",
    "  #    tf.float32,\n",
    "  #    [batch_size, multi_cell.state_size],\n",
    "  #    'initial_state')\n",
    "  \n",
    "    initial_state = create_tuple_placeholders_with_default(\n",
    "      multi_cell.zero_state(batch_size, tf.float32),\n",
    "      extra_dims=(None,),\n",
    "      shape=multi_cell.state_size)      \n",
    "\n",
    "# Embeddings layers.\n",
    "with tf.name_scope('embedding_layer'):\n",
    "    if embedding_size > 0:\n",
    "        embedding = tf.get_variable('embedding', [vocab_size, embedding_size])\n",
    "    else:\n",
    "        embedding = tf.constant(np.eye(vocab_size), dtype=tf.float32)\n",
    "\n",
    "    inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    if is_training and input_dropout > 0:\n",
    "        inputs = tf.nn.dropout(inputs, 1 - input_dropout)\n",
    "\n",
    "with tf.name_scope('slice_inputs'):\n",
    "  # Slice inputs into a list of shape [batch_size, 1] data colums.\n",
    "  sliced_inputs = [tf.squeeze(input_, [1])\n",
    "                   for input_ in tf.split(axis=1, num_or_size_splits=num_unrollings, value=inputs)]\n",
    "\n",
    "# Copy cell to do unrolling and collect outputs.\n",
    "outputs, final_state = tf.contrib.rnn.static_rnn(\n",
    "    multi_cell, sliced_inputs,\n",
    "  initial_state=initial_state)\n",
    "\n",
    "final_state = final_state\n",
    "\n",
    "with tf.name_scope('flatten_ouputs'):\n",
    "  # Flatten the outputs into one dimension.\n",
    "  flat_outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, hidden_size])\n",
    "\n",
    "with tf.name_scope('flatten_targets'):\n",
    "  # Flatten the targets too.\n",
    "  flat_targets = tf.reshape(tf.concat(axis=1, values=targets), [-1])\n",
    "\n",
    "# Create softmax parameters, weights and bias.\n",
    "with tf.variable_scope('softmax') as sm_vs:\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    logits = tf.matmul(flat_outputs, softmax_w) + softmax_b\n",
    "    probs = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "  # Compute mean cross entropy loss for each output.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=flat_targets)\n",
    "    mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "with tf.name_scope('loss_monitor'):\n",
    "  # Count the number of elements and the sum of mean_loss\n",
    "  # from each batch to compute the average loss.\n",
    "    count = tf.Variable(1.0, name='count')\n",
    "    sum_mean_loss = tf.Variable(1.0, name='sum_mean_loss')\n",
    "  \n",
    "    reset_loss_monitor = tf.group(sum_mean_loss.assign(0.0),\n",
    "                                  count.assign(0.0),\n",
    "                                  name='reset_loss_monitor')\n",
    "    update_loss_monitor = tf.group(sum_mean_loss.assign(sum_mean_loss +\n",
    "                                                        mean_loss),\n",
    "                                   count.assign(count + 1),\n",
    "                                   name='update_loss_monitor')\n",
    "    with tf.control_dependencies([update_loss_monitor]):\n",
    "        average_loss = sum_mean_loss / count\n",
    "        ppl = tf.exp(average_loss)\n",
    "\n",
    "# Monitor the loss.\n",
    "loss_summary_name = \"average loss\"\n",
    "ppl_summary_name = \"perplexity\"\n",
    "\n",
    "average_loss_summary = tf.summary.scalar(loss_summary_name, average_loss)\n",
    "ppl_summary = tf.summary.scalar(ppl_summary_name, ppl)\n",
    "\n",
    "# Monitor the loss.\n",
    "summaries = tf.summary.merge([average_loss_summary, ppl_summary],\n",
    "                                  name='loss_monitor')\n",
    "\n",
    "global_step = tf.get_variable('global_step', [],\n",
    "                                   initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "learning_rate = tf.constant(learning_rate)\n",
    "if is_training:\n",
    "  # learning_rate = tf.train.exponential_decay(1.0, self.global_step,\n",
    "  #                                            5000, 0.1, staircase=True)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(mean_loss, tvars),\n",
    "                                      max_grad_norm)\n",
    "  # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # optimizer = tf.train.RMSPropOptimizer(learning_rate, decay_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                         global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 9,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, data_size, batch_generator, is_training,\n",
    "            verbose=0, freq=10, summary_writer=None, debug=False, divide_by_n=1):\n",
    "    \"\"\"Runs the model on the given data for one full pass.\"\"\"\n",
    "# epoch_size = ((data_size // self.batch_size) - 1) // self.num_unrollings\n",
    "    epoch_size = data_size // (batch_size * num_unrollings)\n",
    "    if data_size % (batch_size * num_unrollings) != 0:\n",
    "        epoch_size += 1\n",
    "\n",
    "    if verbose > 0:\n",
    "        logging.info('epoch_size: %d', epoch_size)\n",
    "        logging.info('data_size: %d', data_size)\n",
    "        logging.info('num_unrollings: %d', num_unrollings)\n",
    "        logging.info('batch_size: %d', batch_size)\n",
    "\n",
    "    if is_training:\n",
    "        extra_op = train_op\n",
    "    else:\n",
    "        extra_op = tf.no_op()\n",
    "\n",
    "    # Prepare initial state and reset the average loss\n",
    "    # computation.\n",
    "    state = session.run(zero_state)\n",
    "    reset_loss_monitor.run()\n",
    "    start_time = time.time()\n",
    "    for step in range(epoch_size // divide_by_n):\n",
    "        # Generate the batch and use [:-1] as inputs and [1:] as targets.\n",
    "        data = batch_generator.next()\n",
    "        inputs = np.array(data[:-1]).transpose()\n",
    "        targets = np.array(data[1:]).transpose()\n",
    "\n",
    "        ops = [average_loss, final_state, extra_op,\n",
    "               summaries, global_step, learning_rate]\n",
    "\n",
    "        feed_dict = {input_data: inputs, targets: targets,\n",
    "                     initial_state: state}\n",
    "\n",
    "        results = session.run(ops, feed_dict)\n",
    "        average_loss, state, _, summary_str, global_step, lr = results\n",
    "  \n",
    "        ppl = np.exp(average_loss)\n",
    "        if (verbose > 0) and ((step+1) % freq == 0):\n",
    "            logging.info(\"%.1f%%, step:%d, perplexity: %.3f, speed: %.0f words\",\n",
    "                         (step + 1) * 1.0 / epoch_size * 100, step, ppl,\n",
    "                         (step + 1) * batch_size * num_unrollings /\n",
    "                         (time.time() - start_time))\n",
    "\n",
    "\n",
    "    logging.info(\"Perplexity: %.3f, speed: %.0f words per sec\",\n",
    "                 ppl, (step + 1) * batch_size * num_unrollings /\n",
    "                 (time.time() - start_time))\n",
    "    return ppl, summary_str, global_step\n",
    "\n",
    "def sample_seq(session, length, start_text, vocab_index_dict,\n",
    "             index_vocab_dict, temperature=1.0, max_prob=True):\n",
    "    state = session.run(zero_state)\n",
    "    \n",
    "    # use start_text to warm up the RNN.\n",
    "    if start_text is not None and len(start_text) > 0:\n",
    "        seq = list(start_text)\n",
    "        for char in start_text[:-1]:\n",
    "            x = np.array([[char2id(char, vocab_index_dict)]])\n",
    "            state = session.run(final_state,\n",
    "                                {input_data: x,\n",
    "                                 initial_state: state})\n",
    "        x = np.array([[char2id(start_text[-1], vocab_index_dict)]])\n",
    "    else:\n",
    "        vocab_size = len(vocab_index_dict.keys())\n",
    "        x = np.array([[np.random.randint(0, vocab_size)]])\n",
    "        seq = []\n",
    "    \n",
    "    for i in range(length):\n",
    "        state, logits = session.run([final_state,\n",
    "                                     logits],\n",
    "                                    {input_data: x,\n",
    "                                     initial_state: state})\n",
    "        unnormalized_probs = np.exp((logits - np.max(logits)) / temperature)\n",
    "        probs = unnormalized_probs / np.sum(unnormalized_probs)\n",
    "        \n",
    "        if max_prob:\n",
    "            sample = np.argmax(probs[0])\n",
    "        else:\n",
    "            sample = np.random.choice(vocab_size, 1, p=probs[0])[0]\n",
    "\n",
    "        seq.append(id2char(sample, index_vocab_dict))\n",
    "        x = np.array([[sample]])\n",
    "    return ''.join(seq)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Create the class of batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \"\"\"Generate and hold batches.\"\"\"\n",
    "\n",
    "    def __init__(self, text, batch_size, n_unrollings, vocab_size,\n",
    "                 vocab_index_dict, index_vocab_dict):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self._n_unrollings = n_unrollings\n",
    "        self.vocab_index_dict = vocab_index_dict\n",
    "        self.index_vocab_dict = index_vocab_dict\n",
    "\n",
    "        segment = self._text_size // batch_size\n",
    "\n",
    "        # number of elements in cursor list is the same as\n",
    "        # batch_size.  each batch is just the collection of\n",
    "        # elements in where the cursors are pointing to.\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = char2id(self._text[self._cursor[b]], self.vocab_index_dict)\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._n_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
